%From diagram \ref{fig:diagramSystem}, my system has four main blocks. This chapter is dedicated to describe functionalities of these components as well as the background behind.
This chapter is dedicated to describe briefly theoretical background as well as alternative solutions to subtasks of the project. We will cover from automatic Speech-To-Text, Information Extraction, to Deep Convolutional Neural Network to classify image etc.

\section{Automatic Speech Recognition system}
Automatic Speech Recognition (ASR) system is the core part in any voice command system. Given the audio input, ASR system will output the text form of the input for later processing. Briefly, the audio input is in form of a sound wave over the recording time. It's sampled at a rate of 16000Hz which is enough for recognition. Then this sound wave is chunked at each 20ms and transformed to a spectrogram by using Fourier transform. This then be feeded to a kind of recurrent neural network \cite{Medium:2016}. The output at each time slot is the characters and we normally have a language model to refine this. 

Note that to build an ASR system that performs at the level of Amazon Alexa or Google Now, we need a lot of training data in both quantity (hundreds of thousands hours of spoken audio) and diversity (native and non-native speakers, with and without background noise, etc.) And this kind of data is not available. As the main objective of the project is to use voice to control the robot, not to build a decent ASR system hence I decided to use existing softwares. I explored two solutions: open source systems and Google Cloud.

\subsection{Open-source ASR}
I tried several open-source softwares such as Kaldi \cite{Kaldi:2017}, CMUSphinx \cite{CMUSphinx:2017} and Deep Speech Nervana \cite{DeepSpeech:2017}. As a non-native speaker, I found that they do not perform at an adequate level. For example, I often get "tune rite" when I say "turn right". In addition, it's not easy to use the libraries because long installation processes are required and proper documentations of their usage are limited.

\subsection{Google Cloud Speech API}
My alternative solution is to use a cloud service Google Speech API \cite{GoogleCloud:2017}. Although the system needs to have internet access, it provides qualitative speech recognition and easy to use: we send the audio content and get back the text. By this, we also cut the computation cost of using an ASR server locally. Note that state of the art ASR systems use deep neural network which requires massive computations and resources. For those reasons, I chose to use Google Speech API in my system.

\section{Information Extraction}
\label{sec:InfoExt}
The next task is to extract information from the recognised text. For example, figure \ref{fig:InfoExt} illustrates this process. Given a command, the most important thing is to know which \tbf{intent} it refers to. Secondly, extract the \tbf{properties} (i.e. parameters) that go with it, such as \textit{direction, quantity, etc.} I explored two solutions for this problem: \tit{Part-Of-Speech (POS) tagging} and \tit{string processing}.

\subsection{Part-Of-Speech Tagging}
Part-Of-Speech tagging is the process of assigning words in the given text to a particular part of speech (such as \tit{nouns, verbs, adjectives, adverbs} etc.), based on both definition and contexts. There are various algorithms to solve POS tagging \cite{Jurafsky:2009:SLP:1214993} and there exist available natural language processing software that has POS tagger embedded, such as NLTK \cite{Loper:2002:NNL:1118108.1118117}, Stanford CoreNLP \cite{manning-EtAl:2014:P14-5}. 

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.6\hsize]{./figures/InfoExt}
	\caption{Illustration of information extraction for movement commands.}
	\label{fig:InfoExt}
\end{figure}

The reason why POS tagging can be helpful is that it classifies words into categories so we can pay attention to key words such as \tit{verbs} (tag VB), \tit{nouns} (tag NN or NNS), \tit{adverb} (tag ADV). For example, the verbs in a command are likely to be the \tbf{intent} (turn, move, go, etc.), nouns and adverbs are often \tbf{properties} (left, right, X degrees, Y centimeters, etc.). However, for our specific problem, I found this approach unrobust because of the following reasons:
\begin{itemize}
	\item There is no perfect POS tagger and its performance depends on its training data. Hence, it did not show good result in tagging command sentences (which hardly appear in training corpus).
	\item For movement, people tend to use short commands such as: ”backward X metres”, ”turn left” (the angle is 90 degree implicitly). Unfortunately, many of these short commands do not form a complete sentence and POS taggers can not perform well on incomplete sentences.
\end{itemize}
Figure \ref{fig:POSTagLimits} illustrates the above idea. For example, POS tagger predicts word \tbf{turn} as a noun instead of a verb, and predicts word \tbf{left} as a verb in past tense instead of an adverb. Such misclassifications are frequent and they make the system vunerable. That's the reason why I switched to the alternative approach: string processing.

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.7\hsize]{./figures/POSTagLimits}
	\caption{Limitation of POSTaggers: it does not perform well on sentences that describe commands or orders since those hardly appear in training corpus.}
	\label{fig:POSTagLimits}
\end{figure}

\subsection{String Processing}
Instead of tagging words to categories and dealing with key words (verbs, adverbs, nouns), I just analyse the text by following steps:
\begin{itemize}
	\item Remove redundant words (please, so, well)
	\item Define a synonyms set for my problem and replace synonym words by its representator. For example: \tit{go, move, travel} all map to \tit{go}, \tit{turn, rotate, spin} all map to \tit{turn}, \tit{right, right-side} all map to \tit{right} and so on. By doing this, I have limited and specific words describing \tbf{intents} and \tbf{directions}.
	\item Search for intent words, the directions and check for words describing quantities. Those tokens are in digit format and normally appear at fixed positions corresponding to current intent.
\end{itemize}

This approach is simple but it gives works robustly and consistently for our specific information extraction problem.

\section{Neural Network Overview}
\label{sec:neuralNet}
Neural network is a key technique behind many artifical intelligent systems today. There were already a lot of research about neural network in late 20th century. However, this method only dominates other methods recently (around 2012) thanks to the availability of large datasets and the capability of powerful computation units (GPUs). This section presents briefly fully connected and convolutional neural networks which perform best in image classification. For a more rigorous treatment, please refer to Deep Learning book \cite{Goodfellow-et-al-2016} or chapter 5 of Pattern Recognition and Machine Learning book \cite{Bishop:2006:PRM:1162264}.
\subsection{Fully-connected Neural Network}
A fully connected neural network is a directed graph composed by a input layer, several hidden layers and an output layer. An example is shown in figure \ref{fig:fcNet}: The input data is presented by the input layer (e.g. vector of features of input) and is feeded forward through the network to the output layer which represents the probabilities of input's labels. Between each layers, there are learnable parameters: matrix \textbf{weight $W$} and vector \textbf{bias $b$}. Based on the dimensions of input features, number of classes, and hidden layers' sizes, we can easily determine the dimension of these parameters. 
\begin{figure}[tb]
	\centering
	\includegraphics[width=0.9\hsize]{./figures/fcNet}
	\caption{A simple fully connected neural network which classifies an input data into 3 categories using softmax classifier and rectified linear unit activation function (ReLU).}
	\label{fig:fcNet}
\end{figure}
In practice, fully-connected layers are combined with other blocks and they are usually the final block of a classification model. For example, figure \ref{fig:convNet1} shows a simple convolutional neural network with fully-connected layer at the end to classify the input image.
\begin{figure}[tb]
	\centering
	\includegraphics[width=0.9\hsize]{./figures/convNet1}
	\caption{A simple convolutional neural network: its neurons are transposed into 3D shape (width, height and depth) at each layer. The fully connected layer is placed at the end to classify input.}
	\label{fig:convNet1}
\end{figure}
\subsection{Convolutional Neural Network}
Image inputs usually have a 3D shape: width, height and 3 color channels (RGB). Hence, convolutional neural network also aranges its neurons in 3 dimensions: width ($w$), height ($h$), depth ($h$). Note that the depth of input layer equals the number color channels of input image and the \textbf{depth of convolution layer equals the number of filters applied at that layer}. If we use filters of size $3*3$, then in a convolutional layer $i$:
\begin{itemize}
	\item the weight $W_i$ is a stack of filters and $dim(W_i) = 3*3*d_{i-1}*d_i$ where $d_{i-1}$, $d_i$ are depth of layer $i-1$ and $i$ respectively.
	\item the bias $b_i$ is the vector with length equals number of filters, i.e. $b_i \in R^{d_i}$
	\item We convolve the tensor input $w_{i-1}*h_{i-1}*d_{i-1}$ coming from layer $i-1$ with a filter block $3*3*d_{i-1}*d_{i}$ to obtain tensor size $w_{i-1}*h_{i-1}*d_{i}$. Note that we convolve with padding input to keep the same width and height dimension after convolving.
	\item We add bias $b_i$ element-wise for each "surface" (or "slice") $w_{i-1}*h_{i-1}$ of the current tensor.
	\item We apply activation function ReLU to current tensor.
	\item We sub-sample the current tensor to keep strongest features and reduce tensor size. We have final output tensor of size $w_i * h_i *d_i$
\end{itemize}  
The process is illustrated in figure \ref{fig:convNetsimple}. And a common method for sub-sampling called \tbf{max-pooling} shown in figure \ref{fig:maxpool}. It partitions each "slice" of the tensor into non-overlapping rectangles and choose the maximum value in each rectangle. Hence, we produce non-linearities and prioritise strongest features.
\begin{figure}[tb]
	\centering
	\includegraphics[width=0.9\hsize]{./figures/convNetsimple}
	\caption{Operations and parameters of convolution layers. Note that only input image and 2 convolution layers are drawn here.}
	\label{fig:convNetsimple}
\end{figure}
\begin{figure}[tb]
	\centering
	\includegraphics[width=0.6\hsize]{./figures/maxpool}
	\caption{Max-pooling to reduce tensor size and keep strongest features.}
	\label{fig:maxpool}
\end{figure}

After several convolution layers, we flatten the tensor to put into a fully connected neural network for classification. Note that at these fully connected layers, we often use \textbf{dropout} which is a technique to reduce overfitting at fully connected layers because most of parameters present at these layers (figure \ref{fig:dropout}). In training time, we have a probability (normally $p=0.5$) of dropping out neurons in the fully connected layers from the network and then reinsert the dropped out nodes. We repeat that process for every forward and ackward pass. This will also bring a similar effect as model ensemble. In addition, dropout technique can also be used at convolution layers. For more detail about dropout, please refer to \cite{Srivastava:2014:DSW:2627435.2670313}.
\begin{figure}[tb]
	\centering
	\includegraphics[width=0.8\hsize]{./figures/dropout}
	\caption{Dropout helps to reduce overfitting and ensemble models. Image taken from original paper \cite{Srivastava:2014:DSW:2627435.2670313}.}
	\label{fig:dropout}
\end{figure}

\subsection{Loss Function}
We are left to define a loss function: which allows us to train a machine learning model by updating parameters in order to minimise loss. For classification problems we often use the popular cross-entropy loss function. Consider an input indexed $i$, then its cross-entropy loss is
\begin{align}
\label{form:CELoss}
L_i = -\log(\frac{e^{s^{(i)}_{y_i}}}{\sum_{k=1}^{N}e^{s^{(i)}_k}}) = -s^{(i)}_{y_i} + \log\sum_{k=1}^{N}e^{s^{(i)}_k}
\end{align}
where $y_i$ is its ground-truth label, $s^{(i)}$ denotes its score at the last layer and $N$ is the number of classes (see figure \ref{fig:fcNet}). We can see from formula (\ref{form:CELoss}) that if the score for ground-truth label gets bigger then the loss gets smaller. Next, for each iteration, we apply formula (\ref{form:CELoss}) on a batch of training data $B$ (of cardinal $|B|$):
\begin{align}
L_B = \frac{1}{|B|}\sum_{i \in B} \bigg(-s^{(i)}_{y_i} + \log\sum_{k=1}^{N}e^{s^{(i)}_k}\bigg)
\end{align}
and use stochastic gradient descent \cite{wiki:SGD} to update the parameters accordingly:
\begin{align}
W^{(n+1)}_i &= W^{(n)}_i - \alpha \frac{\partial L_B}{\partial W^{n}_i}\\
b^{(n+1)}_i &= b^{(n)}_i - \alpha \frac{\partial L_B}{\partial b^{n}_i}
\end{align}
where $W_i, b_i$ are parameters of the model (weights and biases), $n$ denotes number of iteration and $\alpha$ is learning rate - a hyperparameter of the model. The gradient is computed by algorithm back-propagation. For more details, please refer to Deep Learning book \cite{Goodfellow-et-al-2016} or chapter 5 of Pattern Recognition and Machine Learning \cite{Bishop:2006:PRM:1162264}.

\subsection{Train, validate and test}
To train and validate a machine learning model, we usually split the dataset into 3 parts: training set ($\approx56\%$), validation set ($\approx14\%$) and test set ($\approx30\%$). We will use training dataset to train the model (in our case they are all the parameters $W_i$, $b_i$) and use the validation dataset to tune other hyperparameters: learning rates, dropout probability, hidden layer sizes, number of filters etc. Once we have done our best on the validation set, we run the model only once on the test dataset to obtain the test accuracy. This figure will be our model performance. 

\section{Image Classification}
\label{sec:imgClass}
\subsection{Problem Description}
\textbf{Image Classification} is the task of assigning an input image to one label from a fixed set of categories. It is directly related to the object finding problem. We will discuss further this point in section \ref{sec:objDetect}. Although the description is short, it is really challenging to achieve a high accuracy ($\geq$ 90\%) because of the following difficulties (figure \ref{fig:ImClasschallenges}):
\begin{enumerate}
	\item Viewpoint variation: the same object can be captured at different camera pose.
	\item Illumination conditions: computers only see the pixel values and minor changes in illumination can result in totally different pixel values. 
	\item Scale variation: the same object can have different sizes in the real world. In addition, the distance of taking photo also cause this variation.
	\item Deformation: many objects are not static hence theirs forms are never unique.
	\item Occlusion: depend on the camera view, sometime only a portion of object is visible.
	\item Background clutter: when object and background are similar
	\item intra-class variation: there are many different types and styles of the same object class (e.g. keys, chairs)
\end{enumerate}

\begin{figure}[tb]
\centering
\includegraphics[width =0.9\hsize]{./figures/ImClasschallenges}
\caption{Some challenges of Image Classification problem. Image source: \cite{cs231n}.}
\label{fig:ImClasschallenges}
\end{figure}
\subsection{State-of-the-art Solution}
Each year, there is an ImageNet Large Scale Visual Recognition Competition (ILSVRC) \cite{ILSVRC15} where research groups from around the world participate and submit their state of the art solution to multiple visual recognition problems. Image Classification is only one task in the challenge. There are 2 evaluations for this task: the top-1 and top-5 accuracies. While top-\tit{n} accuracy is the rate that your top \tit{n} predictions (i.e. the \tit{n} ones with highest probabilities) contain the ground truth label. Please note that, for this task, human beings achive only 5.1\% top-5 error rate, which equals to 94.9\% top-5 accuracy. Since 2012, the best solutions for image classification are always deep convolutional neural networks (CNNs). Figure (\ref{fig:imagenetTop5Err}) illustrates this point. Given the fundamental notions represented in section \ref{sec:neuralNet}, I restate below a brief explaination for the success of CNNs:
\begin{itemize}
	\item Deep neural networks accomodate non-linearity properties through activation layers. This makes the system more flexible and able to prioritise important features.
	\item Each convolutional layer is a stack of filters. After learning (i.e. at test time), those filters can extract from input image many types of features such as: edge, shape, colors etc. (for more details please refer to \cite{DeepVis:2015})
	\item Convolutional layers act as features builder, and given enough data, machines do this job better than human beings (from figure \ref{fig:imagenetTop5Err}, we can see that ResNet with 152 layers reaches 3.6\% error rate).
\end{itemize}
In my experiment, I explored the following models:
\begin{itemize}
	\item VGG16 \cite{DBLP:journals/corr/SimonyanZ14a} - a model of VGG group from Oxford University, one winner of ImageNet competition in 2014.
	\item Resnet50 \cite{DBLP:journals/corr/HeZRS15} - a model from Microsoft research group, winner of ImageNet competition in 2015.
	\item Xception \cite{DBLP:journals/corr/Chollet16a} - a model based on GoogleNet \cite{DBLP:journals/corr/SzegedyVISW15} (winner of ImageNet competition in 2014).
\end{itemize}
Overall, Resnet50 performs the best with better accuracy and shorter computational time.

\begin{figure}[tb]
\centering
\includegraphics[width=0.8\hsize]{./figures/imagenetTop5}
\caption{Top 5 error rate in Imagenet Classification competition from 2010 to 2015. We can note a huge improvement from traditional approaches which use hand-crafted computer vision features (2010, 2011) to deep convolutional neural network (2012). VGG model \cite{DBLP:journals/corr/SimonyanZ14a} is one of the winners in 2014. Its performance nearly equals human beings (5.1\%). Resnet model \cite{DBLP:journals/corr/HeZRS15} is the winner in 2015. \tit{Figure copyright Kaiming He.}}
\label{fig:imagenetTop5Err}
\end{figure}

\subsection{Transfer Learning}
\label{sec:transferLearning}
Transfer Learning is a technique in Machine Learning where knowledge gain during training in one type of problem is used to train in other similar problems. This helps saving time and resource by avoiding training model from scratch. The technique is intensively used in deep neural network models as those normally have a lot of parameters and training from scratch can take weeks. Therefore, to solve our image classification problem, I applied transfer learning on the 3 models discussed in previous subsection. The transfer learning process for image classification problem (figure \ref{fig:transferLearning}) is described as following:
\begin{enumerate}
	\item Initialize the model by using its pretrained weights and biases. Those parameters are provided by their authors.
	\item Cut and replace the fully connected layers of the original model by our \tit{appropriate} fully connected layers. For example, original models classify 1000 objects but our has only 12 categories to classify.
	\item Freeze all layers in convolutional blocks and start training new fully connected layers for a while (10-20 epochs).
	\item Unfreeze final convolutional layers and start fine-tuning model (i.e. training with smaller learning rate).
\end{enumerate}

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.9\hsize]{./figures/transferLearning}
	\caption{Transfer Learning process}
	\label{fig:transferLearning}
\end{figure}

Remind that our problem is to classify objects belongs to 12 categories: \tit{"apple", "pen", "book", "monitor", "mouse", "wallet", "keyboard", "banana", "key", "mug", "pear", "orange"}. They are ordinary things that we see and use daily. To form the dataset, I downloaded 1200 images for each category from ImageNet \cite{imagenet_cvpr09}.

The original VGG16 model is shown in figure \ref{fig:originalVgg16} with details about tensors and filters' shape. On the other hand, one example of applying transfer learning on VGG16 is shown in figure \ref{fig:transferedVgg16}. In this example, we modified the fully connected part and did not fine-tune the final convolutional block. Experimental results about fine-tuning will be given in chapter \ref{chap:ExpRes}.
\begin{figure}[tb]
	\centering
	\includegraphics[width=0.9\hsize]{./figures/originalVgg16}
	\caption{Original VGG16 model. There are 1000 categories to classify.}
	\label{fig:originalVgg16}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.9\hsize]{./figures/transferedVgg16}
	\caption{An example of transfer learning applied on VGG16 model. Note that the fully connected part was modified compared to the original VGG16. In addition there are only 12 categories to classify here.}
	\label{fig:transferedVgg16}
\end{figure}

\section{Camera Model and Distance Estimation}
\label{sec:camModel}
The robot will be introduced more carefully in section \ref{sec:Cozmo}. I only note here that the robot's camera resolution is quite low ($320 \times 240$) and its field of view is also small ($\approx 50 ^{\circ}$). Based on these numbers, I chose to use a simple pinhole camera model that does not involve undistortion. An illustration of the model is shown in figure \ref{fig:pinhole} where $(u, v)$ are \tit{pixel coordinates on the image} and $(x,y,z)$ represent object's position \tit{in the camera's frame}.

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.9\hsize]{./figures/pinhole}
	\caption{Pinhole camera model: note that $(u, v)$ are pixel coordinates on the image, $(x,y,z)$ represent object's position in camera's frame. Image source: \cite{c433}.}
	\label{fig:pinhole}
\end{figure}

In general, we have to take into account that the focal-lengths might differ and the origin of pixel coordinates is the top left corner of the image. Thus, the mathematical formula of the model is the following:
\begin{align}
	\label{form:pinhole}
\begin{bmatrix}
	u \\
	v
\end{bmatrix} = \begin{bmatrix}
f_1 & 0 \\
0 & f_2 
\end{bmatrix} \begin{bmatrix}
x/z\\
y/z
\end{bmatrix} + \begin{bmatrix}
c_1 \\ c_2
\end{bmatrix}
\end{align}

Let's go further by considering a rectangle which is parallel to the image plane at distance $z$. The top-left and bottom-right corners of rectangular are represented in the camera's frame by $(x_1, y_1, z)$ and $(x_2, y_2, z)$ respectively (figure \ref{fig:rectForm}). We can compute the width and height of the rectangle from these coordinates: $w = x_2 - x_1$, $h = y_2 - y_1$. Beside, from formula \ref{form:pinhole}, we have:
\begin{align}
	\label{form:dist0}
	\begin{bmatrix}
		u_1 \\
		v_1
	\end{bmatrix} = \begin{bmatrix}
	f_1 & 0 \\
	0 & f_2 
\end{bmatrix} \begin{bmatrix}
x_1/z\\
y_1/z
\end{bmatrix} + \begin{bmatrix}
c_1 \\ c_2
\end{bmatrix}
\hspace{0.5cm} \text{and} \hspace{0.5cm}
	\begin{bmatrix}
		u_2 \\
		v_2
	\end{bmatrix} = \begin{bmatrix}
	f_1 & 0 \\
	0 & f_2 
\end{bmatrix} \begin{bmatrix}
x_2/z\\
y_2/z
\end{bmatrix} + \begin{bmatrix}
c_1 \\ c_2
\end{bmatrix}
\end{align}

We then deduce:
\begin{align}
	\begin{bmatrix}
		u_2 - u_1\\
		v_2 - v_1
	\end{bmatrix} &= \begin{bmatrix}
	f_1 & 0 \\
	0 & f_2 
\end{bmatrix} \begin{bmatrix}
(x_2 - x_1)/z\\
(y_2 - y_1)/z
\end{bmatrix} \nonumber\\
\label{form:dist1}
\rightarrow u_2 - u_1 &= f_1 \frac{x_2 - x_1}{z} \hspace{0.5cm} \text{and} \hspace{0.5cm} v_2 - v_1 = f_2 \frac{y_2 - y_1}{z} 
\end{align}

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.6\hsize]{./figures/rectForm}
	\caption{A rectange at distance $z$ with top-left and bottom-right corners are represented in the camera's frame by $(x_1, y_1, z)$ and $(x_2, y_2, z)$ respectively. We deduce the width and height of this rectangle from the coordinates.}
	\label{fig:rectForm}
\end{figure}

Note that the image of this rectangular is also a rectangular with width and height in pixel coordinates are: $w_{i} = u_2 - u_1$, $h_{i} = v_2 - v_1$. Combine this with formula \ref{form:dist1} we have:
\begin{align}
\label{form:dist2}
z = f_1 * \frac{w}{w_i} \nonumber \\
z = f_2 * \frac{h}{h_i}
\end{align}
In reality, these two function will not return a same result because of the measure errors as well as the simplicity of the camera model. Hence we could compute the average:
\begin{align}
z = \frac{1}{2} (f_1 * \frac{w}{w_i} + f_2 * \frac{h}{h_i})
\end{align}

Given the real size of the rectangle ($w, h$), the camera's information ($f_1, f_2$) and the image size of the rectangle ($w_i, h_i$), we can deduce the distance $z$. Now, given $z$ computed, we plug it into formula \ref{form:dist0} together with information of the image center ($c_1, c_2$), we can easily derive the position $(x_1, y_1, x_2, y_2)$ of the rectangle:
\begin{align}
	x_1 = \frac{(u_1 - c_1)z}{f_1} \hspace{0.6cm}
	x_2 = \frac{(u_2 - c_1)z}{f_1} \hspace{0.6cm}
	y_1 = \frac{(v_1 - c_2)z}{f_2} \hspace{0.6cm}
	y_2 = \frac{(v_2 - c_2)z}{f_2} 
\end{align}
And then deduce the center of the bounding box:
\begin{align}
x_c = \frac{1}{2}\bigg(\frac{(u_1 - c_1)z}{f_1} + \frac{(u_2 - c_1)z}{f_1} \bigg) \hspace{0.5cm} y_c = \frac{1}{2}\bigg( \frac{(v_1 - c_2)z}{f_2} + \frac{(v_2 - c_2)z}{f_2} \bigg)
\end{align}
All the camera's information mentioned above (focal lengths, image center) can be obtained by doing camera calibration. Fortunately, these specifications are provided by the robot manufacturer. Those details will be mentioned in section \ref{sec:Cozmo}.
This distance estimation capability enables a variety of actions such as localization of the robot in a map with some known fixed tags or autonomous path planning to reach objects.
\section{Object Detection}
\label{sec:objDetect}
\subsection{Problem Description}
Image Classification presented in subsection \ref{sec:imgClass} is challenging, however, in reality, we are likely to deal with even more complex problem: input image contains multiple objects. Furthermore, we would like to know where each object appears in the image. This task is named \textbf{Object Detection} and formally, it can be described as follows:
\begin{itemize}
	\item \tbf{Input}: An image that may contain multiple objects, in terms of both category and quantity. 
	\item \tbf{Output}: The original image with predicted objects. Each object is surrounded by a predicted bounding box.
\end{itemize}
By solving this problem, we can detect multiple objects and also locate them in the image. Knowing the bounding box around the object helps us to estimate the distance from the camera to the objects. This is discussed in section \ref{sec:camModel}.


TODO: order of this section and the camera model section, link these two sections

We will see in the following subsections that the state-of-the-art solutions for Object Detection also rely on a robust image classifier. Hence doing well on Image Classification is a need to build a good object detection system.

\subsection{Evaluation}
The following notions (especially described in our context) are related to the evaluation of an object detection system:
\begin{itemize}
	\item True Positive $T_p$: the detector predicts that a considered label is positive (for example the predicted bounding box contains an object) and the ground-truth label is indeed positive.
	\item False Positive $F_p$: the detector predicts that a considered label is positive while the ground-truth label is negative (i.e. predicted bounding box is just the background)
	\item False Negative $F_n$: the detector predicts that a considered label is negative while the ground-truth shows that it is positive (for example there is indeed an object there but the system fails to detect)
	\item Precision: $P = \frac{T_p}{T_p + F_p}$
	\item Recall: $R = \frac{T_p}{T_p + F_n}$ 
\end{itemize}
Remind that there could be multiple objects located arbitrarily in each image. A high precision ($F_p << T_p$) and low recall ($F_n >> T_p$) system will detect very few objects but most of its predictions are correct. On the other hand, a low precision ($F_p >> T_p$) and high recall ($F_n << T_p$) system will detect a lot of objects but most of its predictions are incorrect. Therefore, a good detector has to have both high precision and high recall.

\subsection{State-of-the-art Solutions}
TODO: evaluation should be clear before solution
general idea
intro faster rcnn

General idea will be



